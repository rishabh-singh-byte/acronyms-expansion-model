####### To run the inferencing on random query generation : ###########
  First run uvicorn server on specific port :
        uvicorn app.main:app --reload --port 8090    or   uvicorn app.main:app --reload 
  
  Run streamlit : 
        streamlit run app/streamlit/app1.py



####### To run only with query on a list of candidate acronyms : #############

    Pass the run_inference.py file with the query[str] instead n [int]
    and change the function name to get_all_model_responses.

    Run uvicorn server on specific port :
        uvicorn app.main:app --reload --port 8090    or   uvicorn app.main:app --reload 
  
    Run streamlit : 
        streamlit run app/streamlit/app.py



######## To run streamlit directly without running the uvicorn server ###########

    Run the streamlit app3.py directly it will be alone sufficient 
    Run streamlit : 
        streamlit run app/streamlit/app3.py


######################  Run vLLM for qwen and LLAMA  #########################

QWEN : 
    vllm serve Qwen/Qwen3-4B-Instruct-2507-FP8 \
        --enable-lora \
        --lora-modules acronym-lora=/home/ubuntu/acronyms_project/checkpoints/final_2 \
        --max-model-len 2048 \
        --gpu-memory-utilization 0.8 \
        --max-num-seqs 32 \
        --block-size 16 \
        --enable-prefix-caching \
        --max-num-batched-tokens 16384 \
        --tensor-parallel-size 1 \
        --pipeline-parallel-size 1 \
        --data-parallel-size 1 \
        --kv-cache-dtype auto \
        --host 0.0.0.0 \
        --port 8000


LLAMA : 
    vllm serve \TinyLlama/TinyLlama-1.1B-Chat-v1.0 \
        --enable-lora \
        --lora-modules acronym-lora=/home/ubuntu/acronyms_project/checkpoints/llama-1.1b-lora_2 \
        --max-model-len 2048 \
        --gpu-memory-utilization 0.8 \
        --max-num-seqs 16 \
        --block-size 16 \
        --enable-prefix-caching \
        --max-num-batched-tokens 8192 \
        --tensor-parallel-size 1 \
        --pipeline-parallel-size 1 \
        --data-parallel-size 1 \
        --kv-cache-dtype auto \
        --host 0.0.0.0 \
        --port 8000
