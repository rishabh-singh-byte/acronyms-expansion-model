{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90e2332a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user\\'s query.\\n\\nInstructions:\\n- Only include expansions that are clearly and directly related to the query\\'s context.\\n- If multiple meanings are relevant, include all of them.\\n- If no acronym is relevant, return an empty dictionary: `{}`.\\n- Acronyms must appear in the query to be considered.\\n- Preserve the acronym casing as it appears in the query.\\n- Output must be a valid **JSON dictionary**:\\n  - Keys: acronyms found in the query.\\n  - Values: lists of relevant expansions (as strings).\\n\\nOutput Format:\\n{\\n  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n}'}]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user's query.\n",
    "\n",
    "Instructions:\n",
    "- Only include expansions that are clearly and directly related to the query's context.\n",
    "- If multiple meanings are relevant, include all of them.\n",
    "- If no acronym is relevant, return an empty dictionary: `{}`.\n",
    "- Acronyms must appear in the query to be considered.\n",
    "- Preserve the acronym casing as it appears in the query.\n",
    "- Output must be a valid **JSON dictionary**:\n",
    "  - Keys: acronyms found in the query.\n",
    "  - Values: lists of relevant expansions (as strings).\n",
    "\n",
    "Output Format:\n",
    "{\n",
    "  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\n",
    "  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def parse_raw_prompt(raw_prompt_string):\n",
    "    \"\"\"Parse raw prompt string into OpenAI-compatible message format for vLLM\"\"\"\n",
    "    parts = [part.strip() for part in raw_prompt_string.split(\"###\") if part.strip()]\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # First part is the system message with instructions\n",
    "    if len(parts) > 0:\n",
    "        # Use simple string content format for vLLM compatibility\n",
    "        messages.append({\"role\": \"system\", \"content\": parts[0]})\n",
    "\n",
    "    # Process examples in pairs (user query, assistant response)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        user_example = parts[i]\n",
    "        assistant_response = parts[i + 1] if i + 1 < len(parts) else \"\"\n",
    "\n",
    "        # Add user example\n",
    "        messages.append({\"role\": \"user\", \"content\": user_example})\n",
    "\n",
    "        # Add assistant response if available\n",
    "        if assistant_response:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    return messages\n",
    "\n",
    "# Parse the system prompt into message format for the API\n",
    "SYSTEM_PROMPT = parse_raw_prompt(SYSTEM_PROMPT)\n",
    "print(SYSTEM_PROMPT[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "627ea010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 16000 training samples and 4000 eval samples in Unsloth format.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your JSON file\n",
    "with open(\"data/best_output_25000.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Format dataset\n",
    "formatted = []\n",
    "for item in data:\n",
    "    query = item[\"Query\"]\n",
    "    candidates = item[\"Candidate_Acronyms\"]  # Assumed to be a list of strings\n",
    "    output = item[\"Best_Output\"]             # Assumed to be a dict\n",
    "\n",
    "    # Format candidates into a readable string\n",
    "    formatted_candidates = json.dumps(candidates, ensure_ascii=False)\n",
    "\n",
    "    # Add each example\n",
    "    formatted.append({\n",
    "        \"instruction\": SYSTEM_PROMPT,\n",
    "        \"input\": f'query: \"{query}\", candidate acronyms: {formatted_candidates}',\n",
    "        \"output\": json.dumps(output, ensure_ascii=False)  # Ensure proper JSON formatting\n",
    "    })\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(formatted)\n",
    "\n",
    "# 60:40 train-eval split\n",
    "train_data, eval_data = train_test_split(formatted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save JSONL files\n",
    "with open(\"data/unsloth_train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\"data/unsloth_eval_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in eval_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Saved {len(train_data)} training samples and {len(eval_data)} eval samples in Unsloth format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3551cb68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 16000 examples [00:00, 371356.04 examples/s]\n",
      "Generating eval split: 4000 examples [00:00, 401234.42 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n",
      "\n",
      "âœ… Sample training example:\n",
      "{'instruction': [{'role': 'system', 'content': 'You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user\\'s query.\\n\\nInstructions:\\n- Only include expansions that are clearly and directly related to the query\\'s context.\\n- If multiple meanings are relevant, include all of them.\\n- If no acronym is relevant, return an empty dictionary: `{}`.\\n- Acronyms must appear in the query to be considered.\\n- Preserve the acronym casing as it appears in the query.\\n- Output must be a valid **JSON dictionary**:\\n  - Keys: acronyms found in the query.\\n  - Values: lists of relevant expansions (as strings).\\n\\nOutput Format:\\n{\\n  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n}'}], 'input': 'query: \"policy irb non-compliance\", candidate acronyms: \"(irb: institutional review board)\"', 'output': '\"{\\\\\"IRB\\\\\": [\\\\\"institutional review board\\\\\"]}\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Paths to your JSONL files\n",
    "train_file = \"/home/ubuntu/acronyms_project/data/unsloth_train_data.jsonl\"\n",
    "eval_file = \"/home/ubuntu/acronyms_project/data/unsloth_eval_data.jsonl\"\n",
    "\n",
    "# Load dataset from JSONL files\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": train_file,\n",
    "    \"eval\": eval_file\n",
    "})\n",
    "\n",
    "# Print basic info\n",
    "print(\"âœ… Dataset loaded:\")\n",
    "print(dataset)\n",
    "print(\"\\nâœ… Sample training example:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055ca633",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1948653/1399384649.py:1: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-08 17:24:13 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.1: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 21.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.10.1 patched 36 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 5,898,240 || all params: 4,028,366,336 || trainable%: 0.1464\n",
      "âœ… Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# Load model + tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,     # Explicit and recommended\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Apply LoRA with Unsloth helper\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    lora_alpha=16,              \n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"], \n",
    ")\n",
    "\n",
    "# Diagnostics\n",
    "model.print_trainable_parameters()\n",
    "print(\"âœ… Model loaded on:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f592418",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 92\u001b[39m\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m formatting_func\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# print(formatting_func(dataset[\"train\"][0]))\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# print(formatting_func(dataset[\"train\"][1]))\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# print(formatting_func(dataset[\"eval\"][0]))\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# assume: tokenizer, dataset, and make_formatting_func from earlier\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m fmt = make_formatting_func(\u001b[43mtokenizer\u001b[49m)\n\u001b[32m     94\u001b[39m row = dataset[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     95\u001b[39m batch = {k: [row[k]] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m]}\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Any, Callable\n",
    "\n",
    "def _clean_output_to_json_str(raw_out: Any) -> str:\n",
    "    if isinstance(raw_out, dict):\n",
    "        return json.dumps(raw_out, ensure_ascii=False)\n",
    "    if not isinstance(raw_out, str):\n",
    "        return json.dumps(raw_out, ensure_ascii=False)\n",
    "    s = raw_out.strip()\n",
    "    if len(s) >= 2 and s[0] == '\"' and s[-1] == '\"':\n",
    "        try:\n",
    "            s = json.loads(s)\n",
    "        except Exception:\n",
    "            s = s[1:-1]\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, str):\n",
    "            obj = json.loads(obj)\n",
    "        if isinstance(obj, (dict, list)):\n",
    "            return json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "def make_formatting_func(tokenizer) -> Callable[[Dict[str, Any]], List[str]]:\n",
    "    def _strip_think(text: Any) -> Any:\n",
    "        if isinstance(text, str):\n",
    "            return re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def _render_one(ex: Dict[str, Any]) -> str:\n",
    "        messages: List[Dict[str, str]] = []\n",
    "\n",
    "        inst = ex.get(\"instruction\")\n",
    "        if isinstance(inst, list):\n",
    "            for m in inst:\n",
    "                if isinstance(m, dict) and \"role\" in m and \"content\" in m:\n",
    "                    messages.append({\"role\": m[\"role\"], \"content\": m[\"content\"]})\n",
    "                elif isinstance(m, str):\n",
    "                    messages.append({\"role\": \"system\", \"content\": m})\n",
    "        elif isinstance(inst, str) and inst.strip():\n",
    "            messages.append({\"role\": \"system\", \"content\": inst})\n",
    "\n",
    "        user_content = ex.get(\"input\", \"\")\n",
    "        if isinstance(user_content, str):\n",
    "            user_content = re.sub(r\"\\bcandidate\\s+acronyms\\b\",\n",
    "                                  \"candidate_acronyms\",\n",
    "                                  user_content,\n",
    "                                  flags=re.IGNORECASE)\n",
    "        else:\n",
    "            user_content = str(user_content)\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        raw_out = _strip_think(ex.get(\"output\", \"\"))\n",
    "        out_clean = _clean_output_to_json_str(raw_out)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": out_clean})\n",
    "\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "\n",
    "    def formatting_func(example: Dict[str, Any]) -> List[str]:\n",
    "        # If Unsloth/TRL passes a single example dict\n",
    "        if not isinstance(example.get(\"input\"), list):\n",
    "            return [_render_one(example)]\n",
    "        # If a batched dict is passed, render each row\n",
    "        size = len(example[\"input\"])\n",
    "        rows = [{k: example[k][i] for k in example} for i in range(size)]\n",
    "        return [_render_one(row) for row in rows]\n",
    "\n",
    "    return formatting_func\n",
    "\n",
    "\n",
    "# print(formatting_func(dataset[\"train\"][0]))\n",
    "# print(formatting_func(dataset[\"train\"][1]))\n",
    "# print(formatting_func(dataset[\"eval\"][0]))\n",
    "# formatted = formatting_func(dataset[\"train\"][0])\n",
    "# print(\"ðŸ§¾ Formatted sample:\\n\", formatted[\"text\"])\n",
    "\n",
    "# assume: tokenizer, dataset, and make_formatting_func from earlier\n",
    "fmt = make_formatting_func(tokenizer)\n",
    "\n",
    "row = dataset[\"train\"][0]\n",
    "batch = {k: [row[k]] for k in [\"instruction\", \"input\", \"output\"]}\n",
    "\n",
    "texts = fmt(batch)\n",
    "print(\"Rendered sample (first 800 chars):\\n\", texts[0][:])\n",
    "print(\"\\nFirst 20 lines:\\n\", \"\\n\".join(texts[0].splitlines()[:]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a09ae53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def make_compute_metrics(tokenizer):\n",
    "    pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds_logits, labels = eval_pred\n",
    "\n",
    "        # Unpack if tuple (logits, ...)\n",
    "        if isinstance(preds_logits, tuple):\n",
    "            preds_logits = preds_logits[0]\n",
    "\n",
    "        # Cast to float32 for stable argmax regardless of fp16/bf16 at eval-time\n",
    "        arr = np.asarray(preds_logits, dtype=np.float32)\n",
    "        if arr.ndim == 3:  # (batch, seq_len, vocab)\n",
    "            pred_ids = arr.argmax(axis=-1)\n",
    "        else:\n",
    "            pred_ids = np.asarray(preds_logits)\n",
    "\n",
    "        # Replace ignore index (-100) before decoding labels\n",
    "        labels = np.asarray(labels)\n",
    "        labels_dec = np.where(labels == -100, pad_id, labels)\n",
    "\n",
    "        # Decode to text\n",
    "        pred_txt = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "        label_txt = tokenizer.batch_decode(labels_dec, skip_special_tokens=True)\n",
    "\n",
    "        # Normalize whitespace\n",
    "        pred_txt = [t.strip() for t in pred_txt]\n",
    "        label_txt = [t.strip() for t in label_txt]\n",
    "\n",
    "        # Exact match (string-level)\n",
    "        em = np.mean([p == l for p, l in zip(pred_txt, label_txt)]).item()\n",
    "\n",
    "        # JSON parse rate (task expects JSON dicts)\n",
    "        def is_json_obj(s):\n",
    "            try:\n",
    "                return isinstance(json.loads(s), dict)\n",
    "            except Exception:\n",
    "                return False\n",
    "        json_ok = np.mean([is_json_obj(p) for p in pred_txt]).item()\n",
    "\n",
    "        # Character-level micro P/R/F1 as a simple robustness check\n",
    "        # (sklearn will compare strings element-wise)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            label_txt, pred_txt, average=\"micro\", zero_division=0\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"exact_match\": float(em),\n",
    "            \"json_parse_rate\": float(json_ok),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1\": float(f1),\n",
    "        }\n",
    "\n",
    "    return compute_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fc1e83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up trainer configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/16000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=12): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:04<00:00, 3518.04 examples/s]\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=12): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:02<00:00, 402.38 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# âœ… Training Configuration & Trainer Setup\n",
    "# ============================================================\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "print(\"Setting up trainer configuration...\")\n",
    "\n",
    "# Training configuration optimized for Unsloth + QLoRA\n",
    "training_args = SFTConfig(\n",
    "    # Output and logging\n",
    "    output_dir=\"./qwen3-4b-qlora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=25,\n",
    "    \n",
    "    # Training parameters\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=-1,  # Let epochs control training length\n",
    "    \n",
    "    # Optimization\n",
    "    learning_rate=3e-4,\n",
    "\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    # Mixed precision\n",
    "    bf16=False,  # Use fp16 for A10G\n",
    "    fp16=True,\n",
    "    # bf16=True,  # Use fp16 for A10G\n",
    "    # fp16=False,\n",
    "    \n",
    "    # Optimizer\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"epoch\",\n",
    "    # eval_steps=100,\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "\n",
    "    dataloader_num_workers=0,  # Set to 0 to avoid multiprocessing issues\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=False,\n",
    "\n",
    "    ddp_find_unused_parameters=False,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    prediction_loss_only=True,  # Only compute loss, not logits\n",
    "    \n",
    "    # SFT specific parameters\n",
    "    dataset_text_field=\"text\",  # This will be handled by formatting_func\n",
    "    max_seq_length=1024,\n",
    "    packing=False,  # Disable packing to avoid length mismatch issues\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# âœ… Eval subset (quick validation)\n",
    "# ============================================================\n",
    "small_eval_dataset = dataset[\"eval\"].select(range(min(1000, len(dataset[\"eval\"]))))\n",
    "\n",
    "# ============================================================\n",
    "# âœ… Trainer setup\n",
    "# ============================================================\n",
    "compute_fn = make_compute_metrics(tokenizer)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    formatting_func=make_formatting_func(tokenizer),  # <- bind tokenizer\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b207bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n",
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 16,000 | Num Epochs = 3 | Total steps = 12,000\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 5,898,240 of 4,028,366,336 (0.15% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 2:59:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.165377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.134100</td>\n",
       "      <td>0.148899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121100</td>\n",
       "      <td>0.146437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12000, training_loss=0.1583680603702863, metrics={'train_runtime': 10767.0397, 'train_samples_per_second': 4.458, 'train_steps_per_second': 1.115, 'total_flos': 2.6766797680944538e+17, 'train_loss': 0.1583680603702863, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7847e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./checkpoints/final_2/tokenizer_config.json',\n",
       " './checkpoints/final_2/special_tokens_map.json',\n",
       " './checkpoints/final_2/chat_template.jinja',\n",
       " './checkpoints/final_2/vocab.json',\n",
       " './checkpoints/final_2/merges.txt',\n",
       " './checkpoints/final_2/added_tokens.json',\n",
       " './checkpoints/final_2/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dir = \"./checkpoints/final_2\"\n",
    "trainer.save_model(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7091cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3ForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.8362035751342773, 'eval_model_preparation_time': 0.0047, 'eval_runtime': 115.8871, 'eval_samples_per_second': 8.629, 'eval_steps_per_second': 8.629}\n",
      "Evaluation Metrics:\n",
      "eval_loss: 2.8362\n",
      "eval_model_preparation_time: 0.0047\n",
      "eval_runtime: 115.8871\n",
      "eval_samples_per_second: 8.6290\n",
      "eval_steps_per_second: 8.6290\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366b4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user's query.\n",
    "\n",
    "Instructions:\n",
    "- Only include expansions that are clearly and directly related to the query's context.\n",
    "- If multiple meanings are relevant, include all of them.\n",
    "- If no acronym is relevant, return an empty dictionary: `{}`.\n",
    "- Acronyms must appear in the query to be considered.\n",
    "- Preserve the acronym casing as it appears in the query.\n",
    "- Output must be a valid **JSON dictionary**:\n",
    "  - Keys: acronyms found in the query.\n",
    "  - Values: lists of relevant expansions (as strings).\n",
    "\n",
    "Output Format:\n",
    "{\n",
    "  \"ACRONYM\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\"]\n",
    "}\n",
    "\n",
    "Examples:\n",
    "###\n",
    "query: \"Who leads the AI team\", candidate acronyms: \" (AI: artificial intelligence, Artificial Intelligence, Action Items)\"\n",
    "###\n",
    "{\"AI\": [\"artificial intelligence\"]}\n",
    "###\n",
    "query: \"update the okr\", candidate acronyms: \" (okr: Objectives and Key Results, Office of Knowledge Research)\"\n",
    "###\n",
    "{\"okr\": [\"Objectives and Key Results\"]}\n",
    "###\n",
    "query: \"follow up with hrbp and cpo\", candidate acronyms: \" (hrbp: Human Resources Business Partner) (cpo: Chief Product Officer, Chief People Officer)\"\n",
    "###\n",
    "{\"hrbp\": [\"Human Resources Business Partner\"], \"cpo\": [\"Chief People Officer\"]}\n",
    "###\n",
    "query: \"can you help me with this\", candidate acronyms: \" (can: Canada) (you: Young Outstanding Undergraduates)\"\n",
    "###\n",
    "{}\n",
    "###\n",
    "\"\"\"\n",
    "def prepare_inference_messages(base_messages_or_system, user_text):\n",
    "    \"\"\"\n",
    "    Normalize to text-only messages for Qwen chat templates that expect strings.\n",
    "    - If given a list of {role, content}, flatten any segment lists to a string.\n",
    "    - If given a plain system string, wrap it as a single system message.\n",
    "    - Append the live user turn as the last message.\n",
    "    \"\"\"\n",
    "    def _to_text(c):\n",
    "        if isinstance(c, list):\n",
    "            parts = []\n",
    "            for seg in c:\n",
    "                if isinstance(seg, dict) and \"text\" in seg:\n",
    "                    parts.append(seg[\"text\"])\n",
    "                else:\n",
    "                    parts.append(str(seg))\n",
    "            return \"\".join(parts)\n",
    "        return str(c)\n",
    "\n",
    "    if isinstance(base_messages_or_system, list):\n",
    "        msgs = []\n",
    "        for m in base_messages_or_system:\n",
    "            role = m.get(\"role\", \"user\")\n",
    "            content = _to_text(m.get(\"content\", \"\"))\n",
    "            msgs.append({\"role\": role, \"content\": content})\n",
    "    else:\n",
    "        msgs = [{\"role\": \"system\", \"content\": str(base_messages_or_system)}]\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "    return msgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff9f41ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Qwen3 patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 21.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:01<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</think>\n",
      "\n",
      "{\"PRO\": [\"professional\"]}\n"
     ]
    }
   ],
   "source": [
    "# minimal_infer_unsloth.py\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "os.environ[\"UNSLOTH_DISABLE_PATCHES\"] = \"1\" \n",
    "\n",
    "model_dir = \"./checkpoints/final_2\"  # set to your directory\n",
    "\n",
    "model, tok = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_dir,\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=False,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "messages = prepare_inference_messages(\n",
    "    SYSTEM_PROMPT,  # <-- your list from training: [{'role':...,'content':...}, ...]\n",
    "    'query: \"pro athelete\", candidate_acronyms: \"(pro: professional, professor)\"'\n",
    ")\n",
    "\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "bad = tok.encode(\"<think>\", add_special_tokens=False)\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    "    pad_token_id=tok.pad_token_id,\n",
    "    bad_words_ids=[bad] if bad else None,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, **{k: v for k, v in gen_kwargs.items() if v is not None})\n",
    "\n",
    "gen_ids = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "text = re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", text).strip()\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-qlora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
