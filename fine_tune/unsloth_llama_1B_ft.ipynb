{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4137ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user\\'s query.\\n\\nInstructions:\\n- Only include expansions that are clearly and directly related to the query\\'s context.\\n- If multiple meanings are relevant, include all of them.\\n- If no acronym is relevant, return an empty dictionary: `{}`.\\n- Acronyms must appear in the query to be considered.\\n- Preserve the acronym casing as it appears in the query.\\n- Output must be a valid **JSON dictionary**:\\n  - Keys: acronyms found in the query.\\n  - Values: lists of relevant expansions (as strings).\\n\\nOutput Format:\\n{\\n  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n}'}]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user's query.\n",
    "\n",
    "Instructions:\n",
    "- Only include expansions that are clearly and directly related to the query's context.\n",
    "- If multiple meanings are relevant, include all of them.\n",
    "- If no acronym is relevant, return an empty dictionary: `{}`.\n",
    "- Acronyms must appear in the query to be considered.\n",
    "- Preserve the acronym casing as it appears in the query.\n",
    "- Output must be a valid **JSON dictionary**:\n",
    "  - Keys: acronyms found in the query.\n",
    "  - Values: lists of relevant expansions (as strings).\n",
    "\n",
    "Output Format:\n",
    "{\n",
    "  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\n",
    "  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def parse_raw_prompt(raw_prompt_string):\n",
    "    \"\"\"Parse raw prompt string into OpenAI-compatible message format for vLLM\"\"\"\n",
    "    parts = [part.strip() for part in raw_prompt_string.split(\"###\") if part.strip()]\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # First part is the system message with instructions\n",
    "    if len(parts) > 0:\n",
    "        # Use simple string content format for vLLM compatibility\n",
    "        messages.append({\"role\": \"system\", \"content\": parts[0]})\n",
    "\n",
    "    # Process examples in pairs (user query, assistant response)\n",
    "    for i in range(1, len(parts), 2):\n",
    "        user_example = parts[i]\n",
    "        assistant_response = parts[i + 1] if i + 1 < len(parts) else \"\"\n",
    "\n",
    "        # Add user example\n",
    "        messages.append({\"role\": \"user\", \"content\": user_example})\n",
    "\n",
    "        # Add assistant response if available\n",
    "        if assistant_response:\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "    return messages\n",
    "\n",
    "# Parse the system prompt into message format for the API\n",
    "SYSTEM_PROMPT = parse_raw_prompt(SYSTEM_PROMPT)\n",
    "print(SYSTEM_PROMPT[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76c16f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 16000 training samples and 4000 eval samples in Unsloth format.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load your JSON file\n",
    "with open(\"data/best_output_25000.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Format dataset\n",
    "formatted = []\n",
    "for item in data:\n",
    "    query = item[\"Query\"]\n",
    "    candidates = item[\"Candidate_Acronyms\"]  # Assumed to be a list of strings\n",
    "    output = item[\"Best_Output\"]             # Assumed to be a dict\n",
    "\n",
    "    # Format candidates into a readable string\n",
    "    formatted_candidates = json.dumps(candidates, ensure_ascii=False)\n",
    "\n",
    "    # Add each example\n",
    "    formatted.append({\n",
    "        \"instruction\": SYSTEM_PROMPT,\n",
    "        \"input\": f'query: \"{query}\", candidate acronyms: {formatted_candidates}',\n",
    "        \"output\": json.dumps(output, ensure_ascii=False)  # Ensure proper JSON formatting\n",
    "    })\n",
    "\n",
    "# Shuffle the dataset\n",
    "random.shuffle(formatted)\n",
    "\n",
    "# 60:40 train-eval split\n",
    "train_data, eval_data = train_test_split(formatted, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save JSONL files\n",
    "with open(\"data/unsloth_train_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with open(\"data/unsloth_eval_data.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in eval_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Saved {len(train_data)} training samples and {len(eval_data)} eval samples in Unsloth format.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4ce6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 16000 examples [00:00, 368553.85 examples/s]\n",
      "Generating eval split: 4000 examples [00:00, 396708.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 16000\n",
      "    })\n",
      "    eval: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 4000\n",
      "    })\n",
      "})\n",
      "\n",
      "âœ… Sample training example:\n",
      "{'instruction': [{'role': 'system', 'content': 'You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user\\'s query.\\n\\nInstructions:\\n- Only include expansions that are clearly and directly related to the query\\'s context.\\n- If multiple meanings are relevant, include all of them.\\n- If no acronym is relevant, return an empty dictionary: `{}`.\\n- Acronyms must appear in the query to be considered.\\n- Preserve the acronym casing as it appears in the query.\\n- Output must be a valid **JSON dictionary**:\\n  - Keys: acronyms found in the query.\\n  - Values: lists of relevant expansions (as strings).\\n\\nOutput Format:\\n{\\n  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\\n}'}], 'input': 'query: \"teams meeting recap\", candidate acronyms: \"(teams: microsoft teams)\"', 'output': '\"{\\\\\"TEAMS\\\\\": [\\\\\"microsoft teams\\\\\"]}\"'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Paths to your JSONL files\n",
    "train_file = \"/home/ubuntu/acronyms_project/data/unsloth_train_data.jsonl\"\n",
    "eval_file = \"/home/ubuntu/acronyms_project/data/unsloth_eval_data.jsonl\"\n",
    "\n",
    "# Load dataset from JSONL files\n",
    "dataset = load_dataset(\"json\", data_files={\n",
    "    \"train\": train_file,\n",
    "    \"eval\": eval_file\n",
    "})\n",
    "\n",
    "# Print basic info\n",
    "print(\"âœ… Dataset loaded:\")\n",
    "print(dataset)\n",
    "print(\"\\nâœ… Sample training example:\")\n",
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b27c9773",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3507592/3215531716.py:1: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 10-11 17:50:56 [__init__.py:216] Automatically detected platform cuda.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 21.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
      "Unsloth 2025.10.1 patched 22 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n",
      "âœ… Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Or any other HF model\n",
    "\n",
    "# Load model and tokenizer in full FP16 precision (no quantization)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,       # Use 16-bit precision\n",
    "    load_in_4bit=False         # Do NOT quantize\n",
    ")\n",
    "\n",
    "# Set pad token if missing\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Apply LoRA with Unsloth helper\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "# Diagnostics\n",
    "model.print_trainable_parameters()\n",
    "print(\"âœ… Model loaded on:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "333c2675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendered sample:\n",
      " <|system|>\n",
      "You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user's query.\n",
      "\n",
      "Instructions:\n",
      "- Only include expansions that are clearly and directly related to the query's context.\n",
      "- If multiple meanings are relevant, include all of them.\n",
      "- If no acronym is relevant, return an empty dictionary: `{}`.\n",
      "- Acronyms must appear in the query to be considered.\n",
      "- Preserve the acronym casing as it appears in the query.\n",
      "- Output must be a valid **JSON dictionary**:\n",
      "  - Keys: acronyms found in the query.\n",
      "  - Values: lists of relevant expansions (as strings).\n",
      "\n",
      "Output Format:\n",
      "{\n",
      "  \"ACRONYM1\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\n",
      "  \"ACRONYM2\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\",...],\n",
      "}</s>\n",
      "<|user|>\n",
      "query: \"teams meeting recap\", candidate_acronyms: \"(teams: microsoft teams)\"</s>\n",
      "<|assistant|>\n",
      "{\"TEAMS\": [\"microsoft teams\"]}</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Any, Callable\n",
    "\n",
    "# Clean up the output field to ensure it's a valid JSON string or fallback to plain string\n",
    "def _clean_output_to_json_str(raw_out: Any) -> str:\n",
    "    if isinstance(raw_out, dict):\n",
    "        return json.dumps(raw_out, ensure_ascii=False)\n",
    "    if not isinstance(raw_out, str):\n",
    "        return json.dumps(raw_out, ensure_ascii=False)\n",
    "\n",
    "    s = raw_out.strip()\n",
    "    if len(s) >= 2 and s[0] == '\"' and s[-1] == '\"':\n",
    "        try:\n",
    "            s = json.loads(s)\n",
    "        except Exception:\n",
    "            s = s[1:-1]\n",
    "\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, str):\n",
    "            obj = json.loads(obj)\n",
    "        if isinstance(obj, (dict, list)):\n",
    "            return json.dumps(obj, ensure_ascii=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return s\n",
    "\n",
    "# Main formatting function for TinyLlama\n",
    "def make_formatting_func(tokenizer) -> Callable[[Dict[str, Any]], List[str]]:\n",
    "    def _strip_think(text: Any) -> Any:\n",
    "        if isinstance(text, str):\n",
    "            return re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def _render_one(ex: Dict[str, Any]) -> str:\n",
    "        messages: List[Dict[str, str]] = []\n",
    "\n",
    "        # System prompt (optional)\n",
    "        system_prompt = None\n",
    "        inst = ex.get(\"instruction\")\n",
    "        if isinstance(inst, str) and inst.strip():\n",
    "            system_prompt = inst.strip()\n",
    "        elif isinstance(inst, list):\n",
    "            for m in inst:\n",
    "                if isinstance(m, dict) and m.get(\"role\") == \"system\":\n",
    "                    system_prompt = m[\"content\"]\n",
    "\n",
    "        if system_prompt:\n",
    "            messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "\n",
    "        # User input\n",
    "        user_content = ex.get(\"input\", \"\")\n",
    "        if isinstance(user_content, str):\n",
    "            user_content = re.sub(r\"\\bcandidate\\s+acronyms\\b\", \"candidate_acronyms\", user_content, flags=re.IGNORECASE)\n",
    "        else:\n",
    "            user_content = str(user_content)\n",
    "\n",
    "        messages.append({\"role\": \"user\", \"content\": user_content})\n",
    "\n",
    "        # Assistant output (label)\n",
    "        raw_out = _strip_think(ex.get(\"output\", \"\"))\n",
    "        out_clean = _clean_output_to_json_str(raw_out)\n",
    "        messages.append({\"role\": \"assistant\", \"content\": out_clean})\n",
    "\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False,\n",
    "        )\n",
    "\n",
    "    def formatting_func(example: Dict[str, Any]) -> List[str]:\n",
    "        if not isinstance(example.get(\"input\"), list):\n",
    "            return [_render_one(example)]\n",
    "\n",
    "        # Batched format\n",
    "        size = len(example[\"input\"])\n",
    "        rows = [{k: example[k][i] for k in example} for i in range(size)]\n",
    "        return [_render_one(row) for row in rows]\n",
    "\n",
    "    return formatting_func\n",
    "\n",
    "fmt = make_formatting_func(tokenizer)\n",
    "\n",
    "row = dataset[\"train\"][0]\n",
    "batch = {k: [row[k]] for k in [\"instruction\", \"input\", \"output\"]}\n",
    "\n",
    "texts = fmt(batch)\n",
    "print(\"Rendered sample:\\n\", texts[0][:])\n",
    "# print(\"\\nFirst 20 lines:\\n\", \"\\n\".join(texts[0].splitlines()[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d0cf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up trainer configuration for TinyLlama...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/16000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=12): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16000/16000 [00:02<00:00, 6178.57 examples/s] \n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=12): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4000/4000 [00:01<00:00, 3112.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# âœ… Training Configuration & Trainer Setup for TinyLlama 1.1B\n",
    "# ============================================================\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from torch.utils.data import Subset\n",
    "import torch\n",
    "\n",
    "print(\"Setting up trainer configuration for TinyLlama...\")\n",
    "\n",
    "# Training configuration optimized for TinyLlama + LoRA (FP16)\n",
    "training_args = SFTConfig(\n",
    "    # Output and logging\n",
    "    output_dir=\"./tinyllama-1.1b-lora\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=25,\n",
    "\n",
    "    # Training parameters\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=3,\n",
    "    max_steps=-1,  # Let epochs control training length\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate=2e-4,  # Slightly lower for stability on smaller models\n",
    "    warmup_steps=100,\n",
    "    max_grad_norm=1.0,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # Mixed precision\n",
    "    bf16=False,\n",
    "    fp16=True,  # Use FP16 for A10G, RTX, or similar\n",
    "\n",
    "    # Optimizer\n",
    "    optim=\"adamw_torch\",  # Better for full-precision / LoRA with small models\n",
    "\n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    "    group_by_length=False,\n",
    "\n",
    "    ddp_find_unused_parameters=False,\n",
    "    gradient_checkpointing=True,\n",
    "\n",
    "    prediction_loss_only=True,\n",
    "\n",
    "    # SFT specific\n",
    "    dataset_text_field=\"text\",  # Will be handled by formatting_func\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# âœ… Trainer setup\n",
    "# ============================================================\n",
    "# compute_fn = make_compute_metrics(tokenizer)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"eval\"],\n",
    "    formatting_func=make_formatting_func(tokenizer),\n",
    "    args=training_args,\n",
    "    # compute_metrics=compute_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a5f912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 16,000 | Num Epochs = 3 | Total steps = 12,000\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 2 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 2,252,800 of 1,102,301,184 (0.20% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12000' max='12000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12000/12000 1:20:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.145900</td>\n",
       "      <td>0.141732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.137700</td>\n",
       "      <td>0.130198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.127442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=12000, training_loss=0.1411018192768097, metrics={'train_runtime': 4802.1369, 'train_samples_per_second': 9.996, 'train_steps_per_second': 2.499, 'total_flos': 9.632824195242394e+16, 'train_loss': 0.1411018192768097, 'epoch': 3.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f9a737b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./checkpoints/llama-1.1b-lora_2/tokenizer_config.json',\n",
       " './checkpoints/llama-1.1b-lora_2/special_tokens_map.json',\n",
       " './checkpoints/llama-1.1b-lora_2/chat_template.jinja',\n",
       " './checkpoints/llama-1.1b-lora_2/tokenizer.model',\n",
       " './checkpoints/llama-1.1b-lora_2/added_tokens.json',\n",
       " './checkpoints/llama-1.1b-lora_2/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dir = \"./checkpoints/llama-1.1b-lora_2\"\n",
    "trainer.save_model(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f3b7286",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a precise assistant tasked with selecting only the **most relevant acronym expansions** from a given list, based strictly on the user's query.\n",
    "\n",
    "Instructions:\n",
    "- Only include expansions that are clearly and directly related to the query's context.\n",
    "- If multiple meanings are relevant, include all of them.\n",
    "- If no acronym is relevant, return an empty dictionary: `{}`.\n",
    "- Acronyms must appear in the query to be considered.\n",
    "- Preserve the acronym casing as it appears in the query.\n",
    "- Output must be a valid **JSON dictionary**:\n",
    "  - Keys: acronyms found in the query.\n",
    "  - Values: lists of relevant expansions (as strings).\n",
    "\n",
    "Output Format:\n",
    "{\n",
    "  \"ACRONYM\": [\"Relevant Expansion 1\", \"Relevant Expansion 2\"]\n",
    "}\n",
    "\n",
    "Examples:\n",
    "###\n",
    "query: \"Who leads the AI team\", candidate acronyms: \" (AI: artificial intelligence, Artificial Intelligence, Action Items)\"\n",
    "###\n",
    "{\"AI\": [\"artificial intelligence\"]}\n",
    "###\n",
    "query: \"update the okr\", candidate acronyms: \" (okr: Objectives and Key Results, Office of Knowledge Research)\"\n",
    "###\n",
    "{\"okr\": [\"Objectives and Key Results\"]}\n",
    "###\n",
    "query: \"follow up with hrbp and cpo\", candidate acronyms: \" (hrbp: Human Resources Business Partner) (cpo: Chief Product Officer, Chief People Officer)\"\n",
    "###\n",
    "{\"hrbp\": [\"Human Resources Business Partner\"], \"cpo\": [\"Chief People Officer\"]}\n",
    "###\n",
    "query: \"can you help me with this\", candidate acronyms: \" (can: Canada) (you: Young Outstanding Undergraduates)\"\n",
    "###\n",
    "{}\n",
    "###\n",
    "\"\"\"\n",
    "def prepare_inference_messages(base_messages_or_system, user_text):\n",
    "    \"\"\"\n",
    "    Normalize to text-only messages for Qwen chat templates that expect strings.\n",
    "    - If given a list of {role, content}, flatten any segment lists to a string.\n",
    "    - If given a plain system string, wrap it as a single system message.\n",
    "    - Append the live user turn as the last message.\n",
    "    \"\"\"\n",
    "    def _to_text(c):\n",
    "        if isinstance(c, list):\n",
    "            parts = []\n",
    "            for seg in c:\n",
    "                if isinstance(seg, dict) and \"text\" in seg:\n",
    "                    parts.append(seg[\"text\"])\n",
    "                else:\n",
    "                    parts.append(str(seg))\n",
    "            return \"\".join(parts)\n",
    "        return str(c)\n",
    "\n",
    "    if isinstance(base_messages_or_system, list):\n",
    "        msgs = []\n",
    "        for m in base_messages_or_system:\n",
    "            role = m.get(\"role\", \"user\")\n",
    "            content = _to_text(m.get(\"content\", \"\"))\n",
    "            msgs.append({\"role\": role, \"content\": content})\n",
    "    else:\n",
    "        msgs = [{\"role\": \"system\", \"content\": str(base_messages_or_system)}]\n",
    "\n",
    "    msgs.append({\"role\": \"user\", \"content\": user_text})\n",
    "    return msgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02090d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.10.1: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.11.0.\n",
      "   \\\\   /|    NVIDIA A10G. Num GPUs = 1. Max memory: 21.988 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 8.6. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver error: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# âœ… Generate response\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# âœ… Decode and clean output\u001b[39;00m\n\u001b[32m     66\u001b[39m gen_ids = out[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/peft/peft_model.py:2007\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2005\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   2006\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2008\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2009\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/unsloth/models/llama.py:1764\u001b[39m, in \u001b[36munsloth_fast_generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# Mixed precision autocast\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.inference_mode(), torch.autocast(device_type = DEVICE_TYPE, dtype = dtype):\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# Return accelerate back\u001b[39;00m\n\u001b[32m   1768\u001b[39m \u001b[38;5;66;03m# if accelerate_new_send_to_device is not None:\u001b[39;00m\n\u001b[32m   1769\u001b[39m \u001b[38;5;66;03m#     accelerate.utils.operations.send_to_device = accelerate_old_send_to_device\u001b[39;00m\n\u001b[32m   1770\u001b[39m \u001b[38;5;66;03m# pass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/transformers/generation/utils.py:2539\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2528\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GenerationMixin.generate(\n\u001b[32m   2529\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2530\u001b[39m         inputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2534\u001b[39m         **kwargs,\n\u001b[32m   2535\u001b[39m     )\n\u001b[32m   2537\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n\u001b[32m   2538\u001b[39m     \u001b[38;5;66;03m# 11. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2539\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2540\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2541\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2542\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2544\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2545\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2546\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2547\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2549\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2550\u001b[39m     \u001b[38;5;66;03m# 11. run beam sample\u001b[39;00m\n\u001b[32m   2551\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._beam_search(\n\u001b[32m   2552\u001b[39m         input_ids,\n\u001b[32m   2553\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2557\u001b[39m         **model_kwargs,\n\u001b[32m   2558\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/transformers/generation/utils.py:2886\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2883\u001b[39m next_token_logits = outputs.logits[:, -\u001b[32m1\u001b[39m, :].to(copy=\u001b[38;5;28;01mTrue\u001b[39;00m, dtype=torch.float32, device=input_ids.device)\n\u001b[32m   2885\u001b[39m \u001b[38;5;66;03m# pre-process distribution\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2886\u001b[39m next_token_scores = \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_token_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2888\u001b[39m \u001b[38;5;66;03m# Store scores, attentions and hidden_states when required\u001b[39;00m\n\u001b[32m   2889\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_in_generate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/transformers/generation/logits_process.py:93\u001b[39m, in \u001b[36mLogitsProcessorList.__call__\u001b[39m\u001b[34m(self, input_ids, scores, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         scores = processor(input_ids, scores, **kwargs)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         scores = \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/qwen-qlora/lib/python3.11/site-packages/transformers/generation/logits_process.py:1196\u001b[39m, in \u001b[36mSequenceBiasLogitsProcessor.__call__\u001b[39m\u001b[34m(self, input_ids, scores)\u001b[39m\n\u001b[32m   1191\u001b[39m     prefix_length = \u001b[38;5;28mlen\u001b[39m(sequence_ids) - \u001b[32m1\u001b[39m\n\u001b[32m   1192\u001b[39m     last_token = sequence_ids[-\u001b[32m1\u001b[39m]\n\u001b[32m   1193\u001b[39m     matching_rows = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43meq\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43mprefix_length\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1197\u001b[39m     bias[:, last_token] += torch.where(\n\u001b[32m   1198\u001b[39m         matching_rows.bool(),\n\u001b[32m   1199\u001b[39m         torch.tensor(sequence_bias, device=input_ids.device),\n\u001b[32m   1200\u001b[39m         torch.tensor(\u001b[32m0.0\u001b[39m, device=input_ids.device),\n\u001b[32m   1201\u001b[39m     )\n\u001b[32m   1203\u001b[39m \u001b[38;5;66;03m# 5 - apply the bias to the scores\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA driver error: invalid argument"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Optional: Disable monkey patches if needed\n",
    "os.environ[\"UNSLOTH_DISABLE_PATCHES\"] = \"1\"\n",
    "\n",
    "# âœ… Path to saved checkpoint (full model or LoRA merged)\n",
    "model_dir = \"./checkpoints/llama-1.1b-lora\"\n",
    "\n",
    "# âœ… Load model & tokenizer\n",
    "model, tok = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_dir,\n",
    "    max_seq_length=2048,\n",
    "    dtype=torch.float16,         # Recommended for inference\n",
    "    load_in_4bit=False           # Assuming you're using FP16 saved model\n",
    ")\n",
    "\n",
    "# âœ… Switch to inference mode (for Unsloth speedup)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# âœ… Set pad token if missing\n",
    "if tok.pad_token_id is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "model.config.pad_token_id = tok.pad_token_id\n",
    "\n",
    "# âœ… Input: define your chat-style input messages\n",
    "# This should follow the same format as during training\n",
    "# Example SYSTEM_PROMPT and user query\n",
    "SYSTEM_PROMPT = \"You are a helpful acronym disambiguation assistant.\"\n",
    "query_text = 'query: \"pro athelete\", candidate_acronyms: \"(pro: professional, professor)\"'\n",
    "\n",
    "# âœ… Function to format chat input\n",
    "def prepare_inference_messages(system_prompt, user_input):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    "\n",
    "# âœ… Prepare prompt\n",
    "messages = prepare_inference_messages(SYSTEM_PROMPT, query_text)\n",
    "prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# âœ… Tokenize input\n",
    "inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# âœ… Prevent <think> token from being generated (if needed)\n",
    "bad = tok.encode(\"<think>\", add_special_tokens=False)\n",
    "bad_words_ids = [bad] if bad else None\n",
    "\n",
    "# âœ… Generation arguments\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tok.eos_token_id,\n",
    "    pad_token_id=tok.pad_token_id,\n",
    "    bad_words_ids=bad_words_ids,\n",
    ")\n",
    "\n",
    "# âœ… Generate response\n",
    "with torch.no_grad():\n",
    "    out = model.generate(**inputs, **gen_kwargs)\n",
    "\n",
    "# âœ… Decode and clean output\n",
    "gen_ids = out[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "text = tok.decode(gen_ids, skip_special_tokens=True)\n",
    "text = re.sub(r\"<think>[\\s\\S]*?</think>\\s*\", \"\", text).strip()\n",
    "\n",
    "print(\"ðŸ§  Model response:\\n\", text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qwen-qlora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
